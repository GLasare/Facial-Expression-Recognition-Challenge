{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GLasare/Facial-Expression-Recognition-Challenge/blob/main/model_exmperiment_XGBOOST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7b504f7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7b504f7a",
        "outputId": "49769493-5067-4bab-a860-9d6c35b3273f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "01ae94a5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01ae94a5",
        "outputId": "beb0d0b6-f970-4166-d45b-1ef86b599d92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "walmart-recruiting-store-sales-forecasting.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "Archive:  walmart-recruiting-store-sales-forecasting.zip\n",
            "replace walmart-recruiting/features.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: walmart-recruiting/features.csv.zip  \n",
            "replace walmart-recruiting/sampleSubmission.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: walmart-recruiting/sampleSubmission.csv.zip  \n",
            "replace walmart-recruiting/stores.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: walmart-recruiting/stores.csv  \n",
            "replace walmart-recruiting/test.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: walmart-recruiting/test.csv.zip  \n",
            "replace walmart-recruiting/train.csv.zip? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: walmart-recruiting/train.csv.zip  \n",
            "Archive:  walmart-recruiting/features.csv.zip\n",
            "replace walmart-recruiting/features.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: walmart-recruiting/features.csv  \n",
            "Archive:  walmart-recruiting/sampleSubmission.csv.zip\n",
            "replace walmart-recruiting/sampleSubmission.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: walmart-recruiting/sampleSubmission.csv  \n",
            "Archive:  walmart-recruiting/test.csv.zip\n",
            "replace walmart-recruiting/test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: walmart-recruiting/test.csv  \n",
            "Archive:  walmart-recruiting/train.csv.zip\n",
            "replace walmart-recruiting/train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: walmart-recruiting/train.csv  \n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/kaggle.json ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c walmart-recruiting-store-sales-forecasting\n",
        "!unzip walmart-recruiting-store-sales-forecasting.zip -d walmart-recruiting\n",
        "\n",
        "!unzip walmart-recruiting/features.csv.zip -d walmart-recruiting/\n",
        "!unzip walmart-recruiting/sampleSubmission.csv.zip -d walmart-recruiting/\n",
        "!unzip walmart-recruiting/test.csv.zip -d walmart-recruiting/\n",
        "!unzip walmart-recruiting/train.csv.zip -d walmart-recruiting/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "pY1owlzAYq0E",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY1owlzAYq0E",
        "outputId": "59b1fef2-81d4-449a-9305-5ed8b4ce79d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 20924\n",
            "drwxr-xr-x 2 root root     4096 Jul  2 12:46 .\n",
            "drwxr-xr-x 1 root root     4096 Jul  2 10:43 ..\n",
            "-rw-r--r-- 1 root root   592289 Feb  3  2014 features.csv\n",
            "-rw-r--r-- 1 root root   161700 Dec 11  2019 features.csv.zip\n",
            "-rw-r--r-- 1 root root  2146739 Feb  3  2014 sampleSubmission.csv\n",
            "-rw-r--r-- 1 root root   225538 Dec 11  2019 sampleSubmission.csv.zip\n",
            "-rw-r--r-- 1 root root      532 Dec 11  2019 stores.csv\n",
            "-rw-r--r-- 1 root root  2598077 Feb  3  2014 test.csv\n",
            "-rw-r--r-- 1 root root   240940 Dec 11  2019 test.csv.zip\n",
            "-rw-r--r-- 1 root root 12842546 Feb  3  2014 train.csv\n",
            "-rw-r--r-- 1 root root  2587161 Dec 11  2019 train.csv.zip\n"
          ]
        }
      ],
      "source": [
        "# should include unzipped data files\n",
        "!ls -la walmart-recruiting/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "L6H9dvLzbnhf",
      "metadata": {
        "id": "L6H9dvLzbnhf",
        "outputId": "8345dc3e-003d-4e45-cd6a-999f18045b94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (337256, 5)\n",
            "Val shape: (84314, 5)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features = pd.read_csv(\"walmart-recruiting/features.csv\")\n",
        "train = pd.read_csv(\"walmart-recruiting/train.csv\")\n",
        "test = pd.read_csv(\"walmart-recruiting/test.csv\")\n",
        "stores = pd.read_csv(\"walmart-recruiting/stores.csv\")\n",
        "\n",
        "train_indices, val_indices = train_test_split(\n",
        "    train.index, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "train_data = train.iloc[train_indices].copy()\n",
        "val_data = train.iloc[val_indices].copy()\n",
        "\n",
        "print(\"Train shape:\", train_data.shape)\n",
        "print(\"Val shape:\", val_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic stats on training data only\n",
        "print(\"\\nSales stats (training only):\")\n",
        "print(train_data['Weekly_Sales'].describe())\n",
        "\n",
        "# Check negatives and zeros\n",
        "print(\"\\nNegative sales:\", (train_data['Weekly_Sales'] < 0).sum())\n",
        "print(\"Zero sales:\", (train_data['Weekly_Sales'] == 0).sum())\n",
        "\n",
        "# Store and dept counts\n",
        "print(\"\\nUnique stores in train:\", train_data['Store'].nunique())\n",
        "print(\"Unique departments in train:\", train_data['Dept'].nunique())\n",
        "\n",
        "# Date range check\n",
        "train_data['Date'] = pd.to_datetime(train_data['Date'])\n",
        "val_data['Date'] = pd.to_datetime(val_data['Date'])\n",
        "print(\"\\nTrain date range:\", train_data['Date'].min(), \"to\", train_data['Date'].max())\n",
        "print(\"Val date range:\", val_data['Date'].min(), \"to\", val_data['Date'].max())\n",
        "\n",
        "# Sample department analysis\n",
        "print(\"\\nSample dept sales (from training):\")\n",
        "for dept in [1, 10, 50, 90]:\n",
        "    dept_sales = train_data[train_data['Dept'] == dept]['Weekly_Sales']\n",
        "    if len(dept_sales) > 0:\n",
        "        print(f\"Dept {dept}: mean=${dept_sales.mean():.2f}, std=${dept_sales.std():.2f}\")\n",
        "\n",
        "# Check features data\n",
        "print(\"\\nFeatures shape:\", features.shape)\n",
        "print(\"Stores shape:\", stores.shape)\n",
        "\n",
        "# Markdown analysis\n",
        "markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "print(\"\\nMarkdown missing percentages:\")\n",
        "for col in markdown_cols:\n",
        "    print(f\"{col}: {features[col].isna().mean() * 100:.1f}%\")\n",
        "\n",
        "# Store types\n",
        "print(\"\\nStore types:\")\n",
        "print(stores['Type'].value_counts())"
      ],
      "metadata": {
        "id": "6CBmVAwDQnsH",
        "outputId": "7c38b3f5-5e18-4635-b9f0-335a8796e549",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "6CBmVAwDQnsH",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sales stats (training only):\n",
            "count    337256.000000\n",
            "mean      15979.221911\n",
            "std       22679.937123\n",
            "min       -4988.940000\n",
            "25%        2080.042500\n",
            "50%        7607.155000\n",
            "75%       20236.550000\n",
            "max      693099.360000\n",
            "Name: Weekly_Sales, dtype: float64\n",
            "\n",
            "Negative sales: 1024\n",
            "Zero sales: 60\n",
            "\n",
            "Unique stores in train: 45\n",
            "Unique departments in train: 81\n",
            "\n",
            "Train date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "Val date range: 2010-02-05 00:00:00 to 2012-10-26 00:00:00\n",
            "\n",
            "Sample dept sales (from training):\n",
            "Dept 1: mean=$19229.72, std=$15485.10\n",
            "Dept 10: mean=$18404.36, std=$13015.94\n",
            "Dept 50: mean=$2644.61, std=$1722.46\n",
            "Dept 90: mean=$45266.02, std=$32427.35\n",
            "\n",
            "Features shape: (8190, 12)\n",
            "Stores shape: (45, 3)\n",
            "\n",
            "Markdown missing percentages:\n",
            "MarkDown1: 50.8%\n",
            "MarkDown2: 64.3%\n",
            "MarkDown3: 55.9%\n",
            "MarkDown4: 57.7%\n",
            "MarkDown5: 50.5%\n",
            "\n",
            "Store types:\n",
            "Type\n",
            "A    22\n",
            "B    17\n",
            "C     6\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataMerger(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, features_df: pd.DataFrame, stores_df: pd.DataFrame):\n",
        "        self.features_df = features_df\n",
        "        self.stores_df = stores_df\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X = X.merge(self.features_df, on=['Store', 'Date'], how='left')\n",
        "        X = X.merge(self.stores_df, on='Store', how='left')\n",
        "        return X"
      ],
      "metadata": {
        "id": "mJquE1vGJ-3A"
      },
      "id": "mJquE1vGJ-3A",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Cleaning**"
      ],
      "metadata": {
        "id": "r6pD5xc9JfNi"
      },
      "id": "r6pD5xc9JfNi"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "eeAoFRJ8zF1e",
      "metadata": {
        "id": "eeAoFRJ8zF1e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class DataCleaner(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, remove_negatives=False, cap_outliers=False, outlier_std_threshold=5):\n",
        "        self.remove_negatives = remove_negatives\n",
        "        self.cap_outliers = cap_outliers\n",
        "        self.outlier_std_threshold = outlier_std_threshold\n",
        "        self.outlier_caps = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.cap_outliers and 'Weekly_Sales' in X.columns:\n",
        "            grouped = X.groupby('Dept')['Weekly_Sales']\n",
        "\n",
        "            for dept, group in grouped:\n",
        "                mean = group.mean()\n",
        "                std = group.std()\n",
        "                upper_cap = mean + self.outlier_std_threshold * std\n",
        "                lower_cap = mean - self.outlier_std_threshold * std\n",
        "                self.outlier_caps[dept] = (lower_cap, upper_cap)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        if self.remove_negatives and 'Weekly_Sales' in X.columns:\n",
        "            X = X[X['Weekly_Sales'] >= 0]\n",
        "\n",
        "        if self.cap_outliers and 'Weekly_Sales' in X.columns:\n",
        "            for dept, (lower_cap, upper_cap) in self.outlier_caps.items():\n",
        "                dept_mask = X['Dept'] == dept\n",
        "                X.loc[dept_mask, 'Weekly_Sales'] = X.loc[dept_mask, 'Weekly_Sales'].clip(\n",
        "                    lower=lower_cap, upper=upper_cap\n",
        "                )\n",
        "\n",
        "        X = X.drop_duplicates(subset=['Store', 'Dept', 'Date'], keep='first')\n",
        "\n",
        "        return X"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MissingValueHandler(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.fill_values = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if 'CPI' in X.columns:\n",
        "            self.fill_values['CPI'] = X.groupby('Store')['CPI'].transform(\n",
        "                lambda x: x.fillna(method='ffill').fillna(method='bfill')\n",
        "            )\n",
        "\n",
        "        if 'Unemployment' in X.columns:\n",
        "            self.fill_values['Unemployment'] = X.groupby('Store')['Unemployment'].transform(\n",
        "                lambda x: x.fillna(method='ffill').fillna(method='bfill')\n",
        "            )\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        if 'CPI' in X.columns and X['CPI'].isna().any():\n",
        "            grouped_cpi = X.groupby('Store')['CPI'].transform(\n",
        "                lambda x: x.fillna(method='ffill').fillna(method='bfill')\n",
        "            )\n",
        "            X['CPI'] = grouped_cpi.fillna(X['CPI'].mean())\n",
        "\n",
        "        if 'Unemployment' in X.columns and X['Unemployment'].isna().any():\n",
        "            grouped_unemp = X.groupby('Store')['Unemployment'].transform(\n",
        "                lambda x: x.fillna(method='ffill').fillna(method='bfill')\n",
        "            )\n",
        "            X['Unemployment'] = grouped_unemp.fillna(X['Unemployment'].mean())\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "WoYb06rsuKNH"
      },
      "id": "WoYb06rsuKNH",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering"
      ],
      "metadata": {
        "id": "2ePr2QHzJl0V"
      },
      "id": "2ePr2QHzJl0V"
    },
    {
      "cell_type": "code",
      "source": [
        "class TemporalFeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X['Date'] = pd.to_datetime(X['Date'])\n",
        "\n",
        "        X['Week'] = X['Date'].dt.isocalendar().week\n",
        "        X['Month'] = X['Date'].dt.month\n",
        "        X['Quarter'] = X['Date'].dt.quarter\n",
        "        X['Year'] = X['Date'].dt.year\n",
        "        X['DayOfYear'] = X['Date'].dt.dayofyear\n",
        "        X['WeekOfMonth'] = (X['Date'].dt.day - 1) // 7 + 1\n",
        "\n",
        "        X['Month_Sin'] = np.sin(2 * np.pi * X['Month'] / 12)\n",
        "        X['Month_Cos'] = np.cos(2 * np.pi * X['Month'] / 12)\n",
        "        X['Week_Sin'] = np.sin(2 * np.pi * X['Week'] / 52)\n",
        "        X['Week_Cos'] = np.cos(2 * np.pi * X['Week'] / 52)\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "rXNUi6bzvDoE"
      },
      "id": "rXNUi6bzvDoE",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LagFeatureEngineer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, lag_weeks=[1, 2, 3, 4, 52],\n",
        "                 rolling_windows=[4, 8, 52]):\n",
        "        self.lag_weeks = lag_weeks\n",
        "        self.rolling_windows = rolling_windows\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X = X.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "        grouped = X.groupby(['Store', 'Dept'])\n",
        "\n",
        "        for lag in self.lag_weeks:\n",
        "            X[f'Sales_Lag_{lag}'] = grouped['Weekly_Sales'].shift(lag)\n",
        "\n",
        "        for window in self.rolling_windows:\n",
        "            X[f'Sales_MA_{window}'] = grouped['Weekly_Sales'].shift(1).rolling(window, min_periods=1).mean()\n",
        "            X[f'Sales_STD_{window}'] = grouped['Weekly_Sales'].shift(1).rolling(window, min_periods=1).std()\n",
        "\n",
        "        X['Sales_Lag_52_YoY_Change'] = (X['Weekly_Sales'] - X['Sales_Lag_52']) / X['Sales_Lag_52']\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "row_paoYIBWJ"
      },
      "id": "row_paoYIBWJ",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class HolidayProximityEngineer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.holidays = {\n",
        "            'SuperBowl': ['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08'],\n",
        "            'LaborDay': ['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06'],\n",
        "            'Thanksgiving': ['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29'],\n",
        "            'Christmas': ['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27']\n",
        "        }\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X['Date_temp'] = pd.to_datetime(X['Date'])\n",
        "\n",
        "        for holiday_name, dates in self.holidays.items():\n",
        "            holiday_dates = pd.to_datetime(dates)\n",
        "\n",
        "            X[f'{holiday_name}_Weeks_Until'] = X['Date_temp'].apply(\n",
        "                lambda x: min([(d - x).days // 7 for d in holiday_dates if d >= x], default=999)\n",
        "            )\n",
        "            X[f'{holiday_name}_Weeks_Since'] = X['Date_temp'].apply(\n",
        "                lambda x: min([(x - d).days // 7 for d in holiday_dates if d <= x], default=999)\n",
        "            )\n",
        "\n",
        "            X[f'{holiday_name}_Within_2_Weeks'] = (\n",
        "                (X[f'{holiday_name}_Weeks_Until'] <= 2) |\n",
        "                (X[f'{holiday_name}_Weeks_Since'] <= 2)\n",
        "            ).astype(int)\n",
        "\n",
        "        X.drop('Date_temp', axis=1, inplace=True)\n",
        "        return X"
      ],
      "metadata": {
        "id": "9l4dZmr9ID0n"
      },
      "id": "9l4dZmr9ID0n",
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DepartmentStoreInteractionEngineer(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        self.store_avg_sales = X.groupby('Store')['Weekly_Sales'].mean()\n",
        "        self.dept_avg_sales = X.groupby('Dept')['Weekly_Sales'].mean()\n",
        "        self.store_dept_avg = X.groupby(['Store', 'Dept'])['Weekly_Sales'].mean()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        X['Store_Avg_Sales'] = X['Store'].map(self.store_avg_sales)\n",
        "        X['Dept_Avg_Sales'] = X['Dept'].map(self.dept_avg_sales)\n",
        "        X['Store_Dept_Avg_Sales'] = X.set_index(['Store', 'Dept']).index.map(self.store_dept_avg)\n",
        "\n",
        "        X['Dept_Sales_Ratio_To_Store'] = X['Store_Dept_Avg_Sales'] / X['Store_Avg_Sales']\n",
        "        X['Store_Size_Dept_Interaction'] = X['Size'] * X['Dept_Sales_Ratio_To_Store']\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "u1uDlu1pIMND"
      },
      "id": "u1uDlu1pIMND",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Selection"
      ],
      "metadata": {
        "id": "8cZj5YwiJs1Y"
      },
      "id": "8cZj5YwiJs1Y"
    },
    {
      "cell_type": "code",
      "source": [
        "class CorrelationFilter(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, threshold=0.8):\n",
        "        self.threshold = threshold\n",
        "        self.to_drop = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        numeric_features = X.select_dtypes(include=[np.number]).columns\n",
        "        corr_matrix = X[numeric_features].corr().abs()\n",
        "\n",
        "        upper_triangle = corr_matrix.where(\n",
        "            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
        "        )\n",
        "\n",
        "        self.to_drop = set()\n",
        "        for column in upper_triangle.columns:\n",
        "            if column in self.to_drop:\n",
        "                continue\n",
        "            correlated_features = list(upper_triangle.index[upper_triangle[column] > self.threshold])\n",
        "            self.to_drop.update(correlated_features)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X.drop(columns=self.to_drop)"
      ],
      "metadata": {
        "id": "297TsjE-JMZK"
      },
      "id": "297TsjE-JMZK",
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ShapFeatureSelector(BaseEstimator, TransformerMixin):\n",
        "   def __init__(self, importance_threshold=0.95, sample_size=1000):\n",
        "       self.importance_threshold = importance_threshold\n",
        "       self.sample_size = sample_size\n",
        "       self.selected_features = None\n",
        "       self.feature_importance_df = None\n",
        "\n",
        "   def fit(self, X, y):\n",
        "       import xgboost as xgb\n",
        "       import shap\n",
        "\n",
        "       excluded_cols = ['Weekly_Sales', 'Date', 'Store', 'Dept']\n",
        "       feature_cols = [col for col in X.columns if col not in excluded_cols]\n",
        "\n",
        "       X_sample = X[feature_cols].sample(n=min(self.sample_size, len(X)), random_state=42)\n",
        "       y_sample = y.loc[X_sample.index]\n",
        "\n",
        "       model = xgb.XGBRegressor(\n",
        "           n_estimators=100,\n",
        "           max_depth=6,\n",
        "           learning_rate=0.1,\n",
        "           random_state=42\n",
        "       )\n",
        "       model.fit(X_sample, y_sample)\n",
        "\n",
        "       explainer = shap.TreeExplainer(model)\n",
        "       shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "       mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
        "\n",
        "       self.feature_importance_df = pd.DataFrame({\n",
        "           'feature': feature_cols,\n",
        "           'importance': mean_abs_shap\n",
        "       }).sort_values('importance', ascending=False)\n",
        "\n",
        "       self.feature_importance_df['importance_normalized'] = (\n",
        "           self.feature_importance_df['importance'] /\n",
        "           self.feature_importance_df['importance'].sum()\n",
        "       )\n",
        "       self.feature_importance_df['cumulative_importance'] = (\n",
        "           self.feature_importance_df['importance_normalized'].cumsum()\n",
        "       )\n",
        "\n",
        "       n_features_to_keep = (\n",
        "           self.feature_importance_df['cumulative_importance'] <= self.importance_threshold\n",
        "       ).sum() + 1\n",
        "\n",
        "       self.selected_features = list(\n",
        "           self.feature_importance_df.head(n_features_to_keep)['feature']\n",
        "       )\n",
        "       self.selected_features.extend(excluded_cols)\n",
        "\n",
        "       return self\n",
        "\n",
        "   def transform(self, X):\n",
        "       return X[self.selected_features]"
      ],
      "metadata": {
        "id": "EbqjlOh7JcW5"
      },
      "id": "EbqjlOh7JcW5",
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def create_xgboost_preprocessing_pipeline(train_df, features_df, stores_df):\n",
        "\n",
        "   pipeline = Pipeline([\n",
        "       ('data_merger', DataMerger(features_df=features_df, stores_df=stores_df)),\n",
        "       ('data_cleaner', DataCleaner(remove_negatives=False, cap_outliers=False)),\n",
        "       ('missing_handler', MissingValueHandler()),\n",
        "       ('temporal_features', TemporalFeatureEngineer()),\n",
        "       ('lag_features', LagFeatureEngineer(lag_weeks=[1, 2, 3, 4, 52],\n",
        "                                          rolling_windows=[4, 8, 52])),\n",
        "       ('holiday_features', HolidayProximityEngineer()),\n",
        "       ('dept_store_features', DepartmentStoreInteractionEngineer()),\n",
        "       ('correlation_filter', CorrelationFilter(threshold=0.95)),\n",
        "       ('shap_selector', ShapFeatureSelector(importance_threshold=0.95, sample_size=5000))\n",
        "   ])\n",
        "\n",
        "   return pipeline\n",
        "\n",
        "def prepare_data_for_xgboost(train_df, features_df, stores_df):\n",
        "\n",
        "   pipeline = create_xgboost_preprocessing_pipeline(train_df, features_df, stores_df)\n",
        "\n",
        "   y = train_df['Weekly_Sales'].copy()\n",
        "\n",
        "   X_transformed = pipeline.fit_transform(train_df, y)\n",
        "\n",
        "   feature_cols = [col for col in X_transformed.columns\n",
        "                  if col not in ['Weekly_Sales', 'Date', 'Store', 'Dept']]\n",
        "\n",
        "   X_final = X_transformed[feature_cols]\n",
        "\n",
        "   return X_final, y, pipeline, feature_cols\n",
        "\n",
        "def transform_test_data(test_df, fitted_pipeline, feature_cols):\n",
        "\n",
        "   X_test_transformed = fitted_pipeline.transform(test_df)\n",
        "   X_test_final = X_test_transformed[feature_cols]\n",
        "\n",
        "   return X_test_final"
      ],
      "metadata": {
        "id": "EbrMcnDJKb3U"
      },
      "id": "EbrMcnDJKb3U",
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Use the same train/val split from analysis\n",
        "train_df = train.iloc[train_indices].copy()\n",
        "val_df = train.iloc[val_indices].copy()\n",
        "\n",
        "# Fit pipeline on training data only\n",
        "X_train_processed, y_train_processed, fitted_pipeline, feature_cols = prepare_data_for_xgboost(\n",
        "    train_df, features, stores\n",
        ")\n",
        "\n",
        "# Transform validation data\n",
        "X_val_processed = transform_test_data(val_df, fitted_pipeline, feature_cols)\n",
        "y_val_processed = val_df['Weekly_Sales']\n",
        "\n",
        "print(\"Processed train shape:\", X_train_processed.shape)\n",
        "print(\"Processed val shape:\", X_val_processed.shape)\n",
        "print(\"Number of features:\", len(feature_cols))\n",
        "\n",
        "# Train model\n",
        "xgb_model = xgb.XGBRegressor(\n",
        "    n_estimators=100,\n",
        "    max_depth=6,\n",
        "    learning_rate=0.1,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nTraining XGBoost...\")\n",
        "xgb_model.fit(X_train_processed, y_train_processed)\n",
        "\n",
        "# Predictions\n",
        "y_pred = xgb_model.predict(X_val_processed)\n",
        "\n",
        "# Get holiday info for WMAE calculation\n",
        "val_with_holidays = val_df.merge(\n",
        "    features[['Store', 'Date', 'IsHoliday']],\n",
        "    on=['Store', 'Date'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "weights = np.where(val_with_holidays['IsHoliday'], 5, 1)\n",
        "wmae = np.sum(weights * np.abs(y_val_processed - y_pred)) / np.sum(weights)\n",
        "\n",
        "print(\"\\nResults:\")\n",
        "print(\"WMAE:\", wmae)\n",
        "print(\"MAE:\", mean_absolute_error(y_val_processed, y_pred))\n",
        "print(\"Holiday weeks in val:\", (weights == 5).sum())\n",
        "print(\"Non-holiday weeks in val:\", (weights == 1).sum())"
      ],
      "metadata": {
        "id": "xRSapUIfKeA9",
        "outputId": "94800813-d28c-4fe6-c2ab-c9d2b90efab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        }
      },
      "id": "xRSapUIfKeA9",
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (335761, 5)\n",
            "Val shape: (85809, 5)\n",
            "Train date range: 2010-02-05 00:00:00 to 2012-04-06 00:00:00\n",
            "Val date range: 2012-04-13 00:00:00 to 2012-10-26 00:00:00\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "You are trying to merge on datetime64[ns] and object columns for key 'Date'. If you wish to proceed you should use pd.concat",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-70-144185791.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Fit pipeline on training data only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m X_train_processed, y_train_processed, fitted_pipeline, feature_cols = prepare_data_for_xgboost(\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m )\n",
            "\u001b[0;32m/tmp/ipython-input-34-3094096236.py\u001b[0m in \u001b[0;36mprepare_data_for_xgboost\u001b[0;34m(train_df, features_df, stores_df)\u001b[0m\n\u001b[1;32m     26\u001b[0m    \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Weekly_Sales'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m    \u001b[0mX_transformed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m    feature_cols = [col for col in X_transformed.columns \n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \"\"\"\n\u001b[1;32m    717\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_method_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    586\u001b[0m             )\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    589\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1549\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m             res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-25-2482006355.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Store'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstores_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Store'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'left'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m  10830\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmerge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 10832\u001b[0;31m         return merge(\n\u001b[0m\u001b[1;32m  10833\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m  10834\u001b[0m             \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    168\u001b[0m         )\n\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         op = _MergeOperation(\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mleft_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mright_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;31m# validate the merge keys dtypes. We may need to coerce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m         \u001b[0;31m# to avoid incompatible dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_coerce_merge_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m         \u001b[0;31m# If argument passed to validate,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/reshape/merge.py\u001b[0m in \u001b[0;36m_maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1510\u001b[0m             \u001b[0;31m# datetimelikes must match exactly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1511\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mneeds_i8_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mneeds_i8_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1512\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1513\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mneeds_i8_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mneeds_i8_conversion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: You are trying to merge on datetime64[ns] and object columns for key 'Date'. If you wish to proceed you should use pd.concat"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RFRG-j_oOmJM"
      },
      "id": "RFRG-j_oOmJM",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}